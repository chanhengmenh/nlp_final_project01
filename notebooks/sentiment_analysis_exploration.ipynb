{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0bbe230",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Analysis: From Baseline to State-of-the-Art"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd0d98",
   "metadata": {},
   "source": [
    "#### This notebook serves as the primary workspace for developing and comparing sentiment analysis models. We will follow a structured approach:\n",
    "1.  **Setup and Data Exploration**: Load libraries and understand the dataset.\n",
    "2.  **Universal Text Preprocessing**: Create a robust cleaning pipeline for our text data.\n",
    "3.  **Part 1: Baseline Models (Scikit-learn)**: Implement and evaluate classic machine learning models using TF-IDF.\n",
    "4.  **Part 2: Deep Learning Models**: Build and evaluate an LSTM model and discuss the implementation of a Transformer (RoBERTa).\n",
    "5.  **Model Comparison & Final Selection**: Compare the results and choose the best model.\n",
    "6.  **Saving and Predicting with the Final Model**: Save the chosen model and use it for inference on new tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3f353",
   "metadata": {},
   "source": [
    "### 1. Setup and Data Exploration: \n",
    "##### First, let's import all necessary libraries, download NLTK data, and perform a brief exploratory data analysis (EDA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ccbf0",
   "metadata": {},
   "source": [
    "#### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15077565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 23:48:55.622193: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-02 23:48:55.624423: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-02 23:48:55.809358: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-02 23:48:56.692325: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-02 23:48:56.692661: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Scikit-learn Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Deep Learning Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Configure plots\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeea75f",
   "metadata": {},
   "source": [
    "#### 1.2 NLTK Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19e1b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data (only needs to be done once)\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK data...\")\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    print(\"Downloads complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c36810",
   "metadata": {},
   "source": [
    "#### 1.3 Data Loading and Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d269e254",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/raw/Tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m os.makedirs(MODEL_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Let's focus on the columns we need: 'text' and 'airline_sentiment'\u001b[39;00m\n\u001b[32m     13\u001b[39m df = df[[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mairline_sentiment\u001b[39m\u001b[33m'\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/raw/Tweets.csv'"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "# Note: The '..' moves one directory up from /notebooks to the project root\n",
    "DATASET_PATH = '../data/raw/Tweets.csv'\n",
    "MODEL_DIR = '../models'\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "# Let's focus on the columns we need: 'text' and 'airline_sentiment'\n",
    "df = df[['text', 'airline_sentiment']]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d54f54",
   "metadata": {},
   "source": [
    "#### 1.4 Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9aad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the count of each sentiment class helps us understand if the dataset is imbalanced.\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='airline_sentiment', data=df, order=['positive', 'neutral', 'negative'])\n",
    "plt.title('Distribution of Sentiments')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b9766d",
   "metadata": {},
   "source": [
    "### 2. Universal Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff063041",
   "metadata": {},
   "source": [
    "##### This is a crucial step to clean the raw text. We will create a single function that will be used across all models to ensure consistency. The pipeline includes:\n",
    "1.  **Regular expression** cleaning\n",
    "2.  **Case normalization**\n",
    "3.  **Tokenization**\n",
    "4.  **Stopwords removals**\n",
    "5.  **Lematization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24aeafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = {'american', 'us', 'airways', 'air', 'airline', 'jetblue', 'virgin', 'united', 'southwest', 'flight'}\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Applies the full text cleaning pipeline to a single string.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Regex Cleaning (remove URLs, mentions, hashtags)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    # 2. Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # 3. Case Normalization\n",
    "    text = text.lower()\n",
    "    # 4. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # 5. Stopwords Removal and Lemmatization\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
    "\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to our text column\n",
    "print(\"Preprocessing text data... (This may take a moment)\")\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "print(\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a9b2b5",
   "metadata": {},
   "source": [
    "### 3. Part 1: Baseline Models (Scikit-learn)\n",
    "##### We will start by training and evaluating several strong baseline models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604700dd",
   "metadata": {},
   "source": [
    "#### 3.1 Feature Extraction (TF-IDF with N-Grams) & Data Splitting\n",
    "##### We convert our cleaned text into numerical features using **TF-IDF Vectorization**. We include **N-Grams** (`ngram_range=(1, 2)`) to capture both single words and two-word phrases, which often carry more meaning than words in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2551d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df['processed_text']\n",
    "y = df['airline_sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Initialize and fit the TF-IDF Vectorizer on the training data\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Shape of TF-IDF matrix for training data: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b1b9e8",
   "metadata": {},
   "source": [
    "#### 3.2 Model Training and Evaluation\n",
    " We will train and evaluate the following Scikit-learn models:\n",
    " - **Support Vector Machine (SVM)**: A powerful model that finds an optimal hyperplane to separate classes.\n",
    " - **Random Forests**: An ensemble method that builds multiple decision trees and merges them to get a more accurate and stable prediction.\n",
    " - **Logistic Regression**: A reliable and interpretable linear model.\n",
    " - **Multinomial Naive Bayes**: A probabilistic model that works very well for text classification.\n",
    "\n",
    " *A Note on K-Means*: K-Means is an **unsupervised clustering** algorithm, meaning it groups data without predefined labels. Since our goal is **supervised classification** (predicting known sentiment labels), K-Means is not an appropriate choice for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models we want to train\n",
    "models = {\n",
    "    \"Linear SVM\": LinearSVC(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB()\n",
    "}\n",
    "\n",
    "# Binarize the labels for AUC calculation\n",
    "y_test_binarized = label_binarize(y_test, classes=['positive', 'neutral', 'negative'])\n",
    "class_labels = ['positive', 'neutral', 'negative']\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"--- Training {name} ---\")\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(f\"\\n--- Evaluation for {name} ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_labels))\n",
    "    \n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_proba = model.predict_proba(X_test_tfidf)\n",
    "    else: # For SVM which uses decision_function\n",
    "        y_pred_proba = model.decision_function(X_test_tfidf)\n",
    "        # We need to reshape for 3-class problem\n",
    "        if len(y_pred_proba.shape) == 1:\n",
    "             y_pred_proba = np.vstack([-y_pred_proba, y_pred_proba]).T\n",
    "    \n",
    "    # Ensure y_pred_proba has 3 columns for 3 classes for AUC calculation\n",
    "    if y_pred_proba.shape[1] == 2 and len(class_labels) == 3:\n",
    "        # A common case for binary classifiers on multi-class data\n",
    "        # We can't calculate multi-class AUC directly, so we'll skip it.\n",
    "        print(\"Skipping Macro-Average AUC for this model.\")\n",
    "    elif y_pred_proba.shape[1] != len(class_labels):\n",
    "         print(\"Skipping Macro-Average AUC due to shape mismatch.\")\n",
    "    else:\n",
    "        auc_score = roc_auc_score(y_test_binarized, y_pred_proba, multi_class='ovr', average='macro')\n",
    "        print(f\"Macro-Average One-vs-Rest AUC: {auc_score:.4f}\\n\")\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=class_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.title(f'Confusion Matrix for {name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "    print(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581fb95e",
   "metadata": {},
   "source": [
    "### 4. Part 2: Deep Learning Models\n",
    "##### Now we'll build more complex models: a Recurrent Neural Network (LSTM) and discuss a state-of-the-art Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0170a9e4",
   "metadata": {},
   "source": [
    "#### 4.1 Advanced Model: RNN/LSTM\n",
    "This requires a different preprocessing pipeline to convert text into sequences of integers for the Embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60287c",
   "metadata": {},
   "source": [
    "#### 4.1.1 Preprocessing for LSTM (Tokenization & Padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db81b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Tokenizer parameters\n",
    "MAX_NB_WORDS = 10000  # Max number of words in the vocabulary\n",
    "MAX_SEQUENCE_LENGTH = 100 # Max length of a tweet\n",
    "EMBEDDING_DIM = 128 # Dimension of the word embeddings\n",
    "\n",
    "# Create and fit the tokenizer\n",
    "keras_tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "keras_tokenizer.fit_on_texts(df['processed_text'].values)\n",
    "\n",
    "# Convert text to sequences and pad them\n",
    "X_seq = keras_tokenizer.texts_to_sequences(df['processed_text'].values)\n",
    "X_pad = pad_sequences(X_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_encoded = pd.get_dummies(df['airline_sentiment']).values\n",
    "\n",
    "# Split the data for the LSTM model\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(\n",
    "    X_pad, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Shape of LSTM training data: {X_train_lstm.shape}\")\n",
    "print(f\"Shape of LSTM training labels: {y_train_lstm.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e41645",
   "metadata": {},
   "source": [
    "#### 4.1.2 Building and Training the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_pad.shape[1]))\n",
    "model_lstm.add(SpatialDropout1D(0.2))\n",
    "model_lstm.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model_lstm.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_lstm.summary())\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model_lstm.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d525b",
   "metadata": {},
   "source": [
    "#### 4.1.3 Evaluating the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "loss, accuracy = model_lstm.evaluate(X_test_lstm, y_test_lstm, verbose=2)\n",
    "print(f\"\\nLSTM Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate classification report and confusion matrix\n",
    "y_pred_lstm_proba = model_lstm.predict(X_test_lstm)\n",
    "y_pred_lstm = np.argmax(y_pred_lstm_proba, axis=1)\n",
    "y_test_labels = np.argmax(y_test_lstm, axis=1)\n",
    "\n",
    "# The order of classes from pd.get_dummies is alphabetical: negative, neutral, positive\n",
    "class_mapping = {0: 'negative', 1: 'neutral', 2: 'positive'} \n",
    "y_pred_lstm_labels = np.vectorize(class_mapping.get)(y_pred_lstm)\n",
    "y_test_actual_labels = np.vectorize(class_mapping.get)(y_test_labels)\n",
    "\n",
    "print(\"\\nLSTM Classification Report:\")\n",
    "print(classification_report(y_test_actual_labels, y_pred_lstm_labels, labels=class_labels))\n",
    "\n",
    "cm_lstm = confusion_matrix(y_test_actual_labels, y_pred_lstm_labels, labels=class_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_lstm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix for LSTM Model')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c79989",
   "metadata": {},
   "source": [
    "### 4.2 State-of-the-Art Model: Transformer (RoBERTa)\n",
    "**Transformers (like RoBERTa)** represent the current state-of-the-art. RoBERTa (A Robustly Optimized BERT Pretraining Approach) improves upon BERT's pre-training strategy, allowing it to often achieve better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b6a255",
   "metadata": {},
   "source": [
    "#### 4.2.1 A Note on Computational Resources\n",
    "**Warning**: Fine-tuning a Transformer model is highly resource-intensive and slow without a GPU. To make this notebook runnable in a standard environment, we will **train on a small subset of the data (1000 samples) for only one epoch**. The resulting accuracy will not be optimal but will serve as a proof-of-concept for the implementation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0af68",
   "metadata": {},
   "source": [
    "#### 4.2.2 Preparing Data for RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ecaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For RoBERTa, we use the original, un-preprocessed text.\n",
    "# The data (X_train, X_test, y_train, y_test) is already split.\n",
    "\n",
    "# Create a smaller subset for demonstration purposes\n",
    "SUBSET_SIZE = 1000\n",
    "X_train_sub = X_train[:SUBSET_SIZE]\n",
    "y_train_sub = y_train[:SUBSET_SIZE]\n",
    "X_test_sub = X_test[:SUBSET_SIZE]\n",
    "y_test_sub = y_test[:SUBSET_SIZE]\n",
    "\n",
    "\n",
    "# Load RoBERTa Tokenizer\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize the data subsets\n",
    "train_encodings = tokenizer_roberta(X_train_sub.tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer_roberta(X_test_sub.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_encoded = pd.get_dummies(y_train_sub).values\n",
    "y_test_encoded = pd.get_dummies(y_test_sub).values\n",
    "\n",
    "# Create TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train_encoded))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b385d0",
   "metadata": {},
   "source": [
    "#### 4.2.3 Loading and Compiling the RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12844150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model_roberta = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "model_roberta.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "print(model_roberta.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb4455",
   "metadata": {},
   "source": [
    "#### 4.2.4 Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model on our subset\n",
    "print(\"\\nFine-tuning RoBERTa model on a subset of data...\")\n",
    "roberta_history = model_roberta.fit(\n",
    "    train_dataset.shuffle(100).batch(16),\n",
    "    epochs=1,\n",
    "    batch_size=16,\n",
    "    validation_data=test_dataset.shuffle(100).batch(16)\n",
    ")\n",
    "print(\"Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96a858",
   "metadata": {},
   "source": [
    "#### 4.2.5 Evaluating the RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9f419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test subset\n",
    "loss_roberta, accuracy_roberta = model_roberta.evaluate(test_dataset.batch(16))\n",
    "print(f\"\\nRoBERTa Model Accuracy on Subset: {accuracy_roberta:.4f}\")\n",
    "\n",
    "# Generate classification report and confusion matrix\n",
    "y_pred_roberta_logits = model_roberta.predict(test_dataset.batch(16)).logits\n",
    "y_pred_roberta = np.argmax(y_pred_roberta_logits, axis=1)\n",
    "y_test_roberta_labels = np.argmax(y_test_encoded, axis=1)\n",
    "\n",
    "\n",
    "y_pred_roberta_mapped = np.vectorize(class_mapping.get)(y_pred_roberta)\n",
    "y_test_actual_roberta_mapped = np.vectorize(class_mapping.get)(y_test_roberta_labels)\n",
    "\n",
    "\n",
    "print(\"\\nRoBERTa Classification Report (on Subset):\")\n",
    "print(classification_report(y_test_actual_roberta_mapped, y_pred_roberta_mapped, labels=class_labels))\n",
    "\n",
    "cm_roberta = confusion_matrix(y_test_actual_roberta_mapped, y_pred_roberta_mapped, labels=class_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_roberta, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix for RoBERTa Model (on Subset)')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b34ea6",
   "metadata": {},
   "source": [
    "### 5. Model Comparison & Final Selection\n",
    "Here we will programmatically compare the results stored from each model run and make a final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1193e8",
   "metadata": {},
   "source": [
    "#### 5.1 Results Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac63d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the results dictionary\n",
    "results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy'])\n",
    "results_df = results_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"--- Model Performance Leaderboard ---\")\n",
    "print(results_df)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Accuracy', y='Model', data=results_df, palette='viridis')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlim(0.5, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa931cab",
   "metadata": {},
   "source": [
    "#### 5.2 Final Decision\n",
    "Let's briefly summarize the results based on the leaderboard:\n",
    "- **Scikit-learn Models**: The `Linear SVM` and `Logistic Regression` models provide excellent baseline accuracies and are extremely fast to train. They are strong contenders for any text classification task.\n",
    "- **LSTM Model**: This model typically achieves a competitive accuracy, demonstrating its ability to understand word order and context, which the TF-IDF models cannot do.\n",
    "- **RoBERTa (on Subset)**: The accuracy on the small subset is for demonstration only and cannot be directly compared. However, it establishes a working pipeline. It is **expected to significantly outperform** all other models when trained on the full dataset with adequate resources (GPU and more epochs).\n",
    "**Decision**: For this project, which aims for a balance of high performance and manageable complexity, the **LSTM model** is the best choice. It delivers strong results without the heavy computational requirements of RoBERTa. If maximum accuracy were the only goal, investing the time and resources to fully train RoBERTa would be the next step.\n",
    "We will select the **LSTM model** as our final, savable artifact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1013066",
   "metadata": {},
   "source": [
    "#### 6. Saving and Predicting with the Final Model\n",
    "We will save the trained LSTM model and its corresponding Keras tokenizer so we can use them for inference without retraining.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9195f89",
   "metadata": {},
   "source": [
    "### 6.1 Saving the Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ab2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define final model paths\n",
    "LSTM_MODEL_PATH = os.path.join(MODEL_DIR, 'final_lstm_model.keras')\n",
    "TOKENIZER_PATH = os.path.join(MODEL_DIR, 'final_keras_tokenizer.pkl')\n",
    "\n",
    "# Save the Keras model and tokenizer\n",
    "model_lstm.save(LSTM_MODEL_PATH)\n",
    "joblib.dump(keras_tokenizer, TOKENIZER_PATH)\n",
    "\n",
    "print(f\"Final Keras Tokenizer saved to: {TOKENIZER_PATH}\")\n",
    "print(f\"Final LSTM Model saved to: {LSTM_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee3abb1",
   "metadata": {},
   "source": [
    "#### 6.2 Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_tweet(text: str):\n",
    "    \"\"\"Loads final model artifacts and predicts sentiment for a new text string.\"\"\"\n",
    "    # Load the saved artifacts\n",
    "    try:\n",
    "        loaded_tokenizer = joblib.load(TOKENIZER_PATH)\n",
    "        loaded_model = tf.keras.models.load_model(LSTM_MODEL_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Model files not found. Please train and save the model first.\")\n",
    "        return\n",
    "\n",
    "    # Preprocess and tokenize the new text\n",
    "    processed_text = preprocess_text(text)\n",
    "    sequence = loaded_tokenizer.texts_to_sequences([processed_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    # Predict\n",
    "    prediction_proba = loaded_model.predict(padded_sequence)[0]\n",
    "    \n",
    "    # Get class with highest probability\n",
    "    class_labels = ['negative', 'neutral', 'positive'] # Alphabetical order from get_dummies\n",
    "    prediction_label = class_labels[np.argmax(prediction_proba)]\n",
    "    probabilities = dict(zip(class_labels, prediction_proba))\n",
    "\n",
    "    print(f\"\\nTweet: '{text}'\")\n",
    "    print(f\"Predicted Sentiment: -> {prediction_label} <-\")\n",
    "    print(\"Probabilities:\")\n",
    "    for sentiment, prob in probabilities.items():\n",
    "        print(f\"  - {sentiment}: {prob:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d07f2",
   "metadata": {},
   "source": [
    "### 6.3 Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189d564",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpredict_new_tweet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mI am so happy with their service, it was an amazing journey!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m predict_new_tweet(\u001b[33m\"\u001b[39m\u001b[33mThe plane was dirty and the staff was rude. Never flying with them again.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m predict_new_tweet(\u001b[33m\"\u001b[39m\u001b[33mMy flight from JFK to LAX is on time.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mpredict_new_tweet\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load the saved artifacts\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     loaded_tokenizer = \u001b[43mjoblib\u001b[49m.load(TOKENIZER_PATH)\n\u001b[32m      6\u001b[39m     loaded_model = tf.keras.models.load_model(LSTM_MODEL_PATH)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "predict_new_tweet(\"I am so happy with their service, it was an amazing journey!\")\n",
    "predict_new_tweet(\"The plane was dirty and the staff was rude. Never flying with them again.\")\n",
    "predict_new_tweet(\"My flight from JFK to LAX is on time.\")\n",
    "predict_new_tweet(\"@AmericanAir you are the worst. My flight is delayed again!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
