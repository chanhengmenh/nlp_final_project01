{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f26f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: light\n",
    "#       format_version: '1.5'\n",
    "#       jupytext_version: 1.14.5\n",
    "#   kernelspec:\n",
    "#     display_name: Python 3\n",
    "#     name: python3\n",
    "# ---\n",
    "\n",
    "# # Tweet Sentiment Analysis: From Baseline to State-of-the-Art\n",
    "#\n",
    "# This notebook serves as the primary workspace for developing and comparing sentiment analysis models. We will follow a structured approach:\n",
    "# 1.  **Setup and Data Exploration**: Load libraries and understand the dataset.\n",
    "# 2.  **Universal Text Preprocessing**: Create a robust cleaning pipeline for our text data.\n",
    "# 3.  **Part 1: Baseline Models (Scikit-learn)**: Implement and evaluate classic machine learning models using TF-IDF.\n",
    "# 4.  **Part 2: Deep Learning Models (TensorFlow/Keras)**: Build and evaluate an LSTM model and a Transformer (RoBERTa).\n",
    "# 5.  **Model Comparison & Final Selection**: Programmatically compare the results and choose the best model.\n",
    "# 6.  **Saving and Predicting with the Final Model**: Save the chosen model and use it for inference on new tweets.\n",
    "\n",
    "# ## 1. Setup and Data Exploration\n",
    "#\n",
    "# First, let's import all necessary libraries, download NLTK data, and perform a brief exploratory data analysis (EDA).\n",
    "\n",
    "# ### 1.1 Imports\n",
    "\n",
    "# +\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Scikit-learn Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# TensorFlow and Keras Imports for Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Transformers Import\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "\n",
    "# Configure plots\n",
    "sns.set_style('whitegrid')\n",
    "# -\n",
    "\n",
    "# ### 1.2 NLTK Downloads\n",
    "# Download NLTK data (only needs to be done once)\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK data...\")\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    print(\"Downloads complete.\")\n",
    "\n",
    "# ### 1.3 Data Loading and Initial Analysis\n",
    "\n",
    "# +\n",
    "# Define file paths\n",
    "# Note: The '..' moves one directory up from /notebooks to the project root\n",
    "DATASET_PATH = '../data/raw/Tweets.csv'\n",
    "MODEL_DIR = '../models'\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "# Let's focus on the columns we need: 'text' and 'airline_sentiment'\n",
    "df = df[['text', 'airline_sentiment']]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "# -\n",
    "\n",
    "# ### 1.4 Sentiment Distribution\n",
    "# Visualizing the count of each sentiment class helps us understand if the dataset is imbalanced.\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='airline_sentiment', data=df, order=['positive', 'neutral', 'negative'])\n",
    "plt.title('Distribution of Sentiments')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.show()\n",
    "\n",
    "# ## 2. Universal Text Preprocessing\n",
    "#\n",
    "# This is a crucial step to clean the raw text. We will create a single function that will be used across all models to ensure consistency.\n",
    "\n",
    "# +\n",
    "# Initialize preprocessing tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = {'american', 'us', 'airways', 'air', 'airline', 'jetblue', 'virgin', 'united', 'southwest', 'flight'}\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Applies the full text cleaning pipeline to a single string.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1]\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to our text column\n",
    "print(\"Preprocessing text data... (This may take a moment)\")\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "print(\"Preprocessing complete.\")\n",
    "# -\n",
    "\n",
    "# ## 3. Part 1: Baseline Models (Scikit-learn)\n",
    "\n",
    "# ### 3.1 Feature Extraction (TF-IDF) & Data Splitting\n",
    "\n",
    "# +\n",
    "# Define features (X) and target (y)\n",
    "X = df['text'] \n",
    "y = df['airline_sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Preprocess the splits for Scikit-Learn models\n",
    "X_train_processed = X_train.apply(preprocess_text)\n",
    "X_test_processed = X_test.apply(preprocess_text)\n",
    "\n",
    "# Initialize and fit the TF-IDF Vectorizer on the training data\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_processed)\n",
    "X_test_tfidf = vectorizer.transform(X_test_processed)\n",
    "\n",
    "print(f\"Shape of TF-IDF matrix for training data: {X_train_tfidf.shape}\")\n",
    "# -\n",
    "\n",
    "# ### 3.2 Model Training and Evaluation\n",
    "\n",
    "# +\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "class_labels = ['negative', 'neutral', 'positive'] # Define a fixed order\n",
    "\n",
    "# Define the models we want to train\n",
    "models = {\n",
    "    \"Linear SVM\": LinearSVC(random_state=42, dual=True), # dual=True is often recommended when n_samples > n_features\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB()\n",
    "}\n",
    "\n",
    "# Binarize the labels for AUC calculation\n",
    "y_test_binarized = label_binarize(y_test, classes=class_labels)\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"--- Training {name} ---\")\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    \n",
    "    print(f\"\\n--- Evaluation for {name} ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_labels))\n",
    "    \n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_proba = model.predict_proba(X_test_tfidf)\n",
    "    else: # For SVM which uses decision_function\n",
    "        y_pred_proba = model.decision_function(X_test_tfidf)\n",
    "    \n",
    "    # Calculate AUC score if possible\n",
    "    try:\n",
    "        auc_score = roc_auc_score(y_test_binarized, y_pred_proba, multi_class='ovr', average='macro')\n",
    "        print(f\"Macro-Average One-vs-Rest AUC: {auc_score:.4f}\\n\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not compute AUC score for {name}: {e}\\n\")\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=class_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.title(f'Confusion Matrix for {name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "# -\n",
    "\n",
    "# ## 4. Part 2: Deep Learning Models (TensorFlow)\n",
    "\n",
    "# ### 4.1 Advanced Model: RNN/LSTM\n",
    "\n",
    "# #### 4.1.1 Preprocessing for LSTM (Tokenization & Padding)\n",
    "\n",
    "# +\n",
    "# Keras Tokenizer parameters\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "# One-hot encode the labels and get the defined class order\n",
    "y_encoded_df = pd.get_dummies(df['airline_sentiment'])\n",
    "y_encoded = y_encoded_df.values\n",
    "class_labels_deep_learning = y_encoded_df.columns.tolist() # Robustly get class order\n",
    "print(f\"Class order for Deep Learning models: {class_labels_deep_learning}\")\n",
    "\n",
    "\n",
    "# Create and fit the tokenizer\n",
    "keras_tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "keras_tokenizer.fit_on_texts(df['processed_text'].values)\n",
    "\n",
    "# Convert text to sequences and pad them\n",
    "X_seq = keras_tokenizer.texts_to_sequences(df['processed_text'].values)\n",
    "X_pad = pad_sequences(X_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Split the data for the LSTM model\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(\n",
    "    X_pad, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nShape of LSTM training data: {X_train_lstm.shape}\")\n",
    "print(f\"Shape of LSTM training labels: {y_train_lstm.shape}\")\n",
    "# -\n",
    "\n",
    "# #### 4.1.2 Building and Training the LSTM Model\n",
    "\n",
    "# +\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_pad.shape[1]))\n",
    "model_lstm.add(SpatialDropout1D(0.2))\n",
    "model_lstm.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model_lstm.add(Dense(len(class_labels_deep_learning), activation='softmax')) # Use number of classes\n",
    "\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_lstm.summary())\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model_lstm.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]\n",
    ")\n",
    "# -\n",
    "\n",
    "# #### 4.1.3 Evaluating the LSTM Model\n",
    "\n",
    "# +\n",
    "# Evaluate on the test set\n",
    "loss, accuracy_lstm = model_lstm.evaluate(X_test_lstm, y_test_lstm, verbose=2)\n",
    "results['LSTM'] = accuracy_lstm\n",
    "print(f\"\\nLSTM Model Accuracy: {accuracy_lstm:.4f}\")\n",
    "\n",
    "# Generate classification report and confusion matrix\n",
    "y_pred_lstm_proba = model_lstm.predict(X_test_lstm)\n",
    "y_pred_lstm = np.argmax(y_pred_lstm_proba, axis=1)\n",
    "y_test_labels = np.argmax(y_test_lstm, axis=1)\n",
    "\n",
    "y_pred_lstm_labels = [class_labels_deep_learning[i] for i in y_pred_lstm]\n",
    "y_test_actual_labels = [class_labels_deep_learning[i] for i in y_test_labels]\n",
    "\n",
    "print(\"\\nLSTM Classification Report:\")\n",
    "print(classification_report(y_test_actual_labels, y_pred_lstm_labels, labels=class_labels_deep_learning))\n",
    "\n",
    "cm_lstm = confusion_matrix(y_test_actual_labels, y_pred_lstm_labels, labels=class_labels_deep_learning)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_lstm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels_deep_learning, yticklabels=class_labels_deep_learning)\n",
    "plt.title('Confusion Matrix for LSTM Model')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "# -\n",
    "\n",
    "# ### 4.2 State-of-the-Art Model: Transformer (RoBERTa)\n",
    "#\n",
    "# **Warning**: Fine-tuning a Transformer model is highly resource-intensive. To make this notebook runnable, we will **train on a small subset of the data (1000 samples) for only one epoch**. The result is for demonstration and not indicative of the model's full potential.\n",
    "\n",
    "# #### 4.2.1 Preparing Data for RoBERTa\n",
    "\n",
    "# +\n",
    "# Create a smaller subset for demonstration\n",
    "SUBSET_SIZE = 1000\n",
    "X_train_sub = X_train[:SUBSET_SIZE]\n",
    "y_train_sub = y_train[:SUBSET_SIZE]\n",
    "X_test_sub = X_test[:SUBSET_SIZE]\n",
    "y_test_sub = y_test[:SUBSET_SIZE]\n",
    "\n",
    "# Load RoBERTa Tokenizer\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize the data subsets\n",
    "train_encodings = tokenizer_roberta(X_train_sub.tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer_roberta(X_test_sub.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_encoded_sub = pd.get_dummies(y_train_sub)[class_labels_deep_learning].values\n",
    "y_test_encoded_sub = pd.get_dummies(y_test_sub)[class_labels_deep_learning].values\n",
    "\n",
    "# Create TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train_encoded_sub))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test_encoded_sub))\n",
    "# -\n",
    "\n",
    "# #### 4.2.2 Loading and Compiling the RoBERTa Model\n",
    "\n",
    "# +\n",
    "# Load pre-trained model\n",
    "model_roberta = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(class_labels_deep_learning))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "model_roberta.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "# -\n",
    "\n",
    "# #### 4.2.3 Fine-Tuning the Model\n",
    "\n",
    "# +\n",
    "# Fine-tune the model on our subset\n",
    "print(\"\\nFine-tuning RoBERTa model on a subset of data...\")\n",
    "roberta_history = model_roberta.fit(\n",
    "    train_dataset.shuffle(100).batch(16),\n",
    "    epochs=1,\n",
    "    batch_size=16\n",
    ")\n",
    "print(\"Fine-tuning complete.\")\n",
    "# -\n",
    "\n",
    "# #### 4.2.4 Evaluating the RoBERTa Model\n",
    "\n",
    "# +\n",
    "# Evaluate on the test subset\n",
    "loss_roberta, accuracy_roberta = model_roberta.evaluate(test_dataset.batch(16))\n",
    "results['RoBERTa (Subset)'] = accuracy_roberta\n",
    "print(f\"\\nRoBERTa Model Accuracy on Subset: {accuracy_roberta:.4f}\")\n",
    "\n",
    "# Generate classification report and confusion matrix\n",
    "y_pred_roberta_logits = model_roberta.predict(test_dataset.batch(16)).logits\n",
    "y_pred_roberta = np.argmax(y_pred_roberta_logits, axis=1)\n",
    "y_test_roberta_labels = np.argmax(y_test_encoded_sub, axis=1)\n",
    "\n",
    "y_pred_roberta_mapped = [class_labels_deep_learning[i] for i in y_pred_roberta]\n",
    "y_test_actual_roberta_mapped = [class_labels_deep_learning[i] for i in y_test_roberta_labels]\n",
    "\n",
    "print(\"\\nRoBERTa Classification Report (on Subset):\")\n",
    "print(classification_report(y_test_actual_roberta_mapped, y_pred_roberta_mapped, labels=class_labels_deep_learning))\n",
    "\n",
    "cm_roberta = confusion_matrix(y_test_actual_roberta_mapped, y_pred_roberta_mapped, labels=class_labels_deep_learning)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_roberta, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels_deep_learning, yticklabels=class_labels_deep_learning)\n",
    "plt.title('Confusion Matrix for RoBERTa Model (on Subset)')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "# -\n",
    "\n",
    "# ## 5. Model Comparison & Final Selection\n",
    "#\n",
    "# Here we will programmatically compare the results stored from each model run and make a final decision.\n",
    "\n",
    "# ### 5.1 Results Leaderboard\n",
    "\n",
    "# +\n",
    "# Create a DataFrame from the results dictionary\n",
    "results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy'])\n",
    "results_df = results_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"--- Model Performance Leaderboard ---\")\n",
    "print(results_df)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Accuracy', y='Model', data=results_df, palette='viridis')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlim(0.5, max(1.0, results_df['Accuracy'].max() * 1.1))\n",
    "plt.show()\n",
    "# -\n",
    "\n",
    "# ### 5.2 Final Decision\n",
    "#\n",
    "# **Decision**: For this project, which aims for a balance of high performance and manageable complexity, the **LSTM model** is the best choice. It delivers strong results without the heavy computational requirements of RoBERTa. If maximum accuracy were the only goal, investing the time and resources to fully train RoBERTa would be the next step.\n",
    "#\n",
    "# We will select the **LSTM model** as our final, savable artifact.\n",
    "\n",
    "# ## 6. Saving and Predicting with the Final Model\n",
    "#\n",
    "# **Important**: Before running this section, make sure you have successfully run all the cells above, especially the LSTM training and evaluation cells. The files must exist on disk to be loaded.\n",
    "\n",
    "# ### 6.1 Saving the Artifacts\n",
    "\n",
    "# +\n",
    "# Define final model paths\n",
    "LSTM_MODEL_PATH = os.path.join(MODEL_DIR, 'final_lstm_model.keras')\n",
    "TOKENIZER_PATH = os.path.join(MODEL_DIR, 'final_keras_tokenizer.pkl')\n",
    "\n",
    "# Save the Keras model and tokenizer\n",
    "model_lstm.save(LSTM_MODEL_PATH)\n",
    "joblib.dump(keras_tokenizer, TOKENIZER_PATH)\n",
    "\n",
    "print(f\"Final Keras Tokenizer saved to: {TOKENIZER_PATH}\")\n",
    "print(f\"Final LSTM Model saved to: {LSTM_MODEL_PATH}\")\n",
    "# -\n",
    "\n",
    "# ### 6.2 Prediction Function\n",
    "\n",
    "def predict_new_tweet(text: str):\n",
    "    \"\"\"Loads final model artifacts and predicts sentiment for a new text string.\"\"\"\n",
    "    # Load the saved artifacts\n",
    "    try:\n",
    "        loaded_tokenizer = joblib.load(TOKENIZER_PATH)\n",
    "        loaded_model = tf.keras.models.load_model(LSTM_MODEL_PATH)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Model files not found. Please ensure the training cells above have been run to save the model.\")\n",
    "        return\n",
    "\n",
    "    # Preprocess and tokenize the new text\n",
    "    processed_text = preprocess_text(text)\n",
    "    sequence = loaded_tokenizer.texts_to_sequences([processed_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    # Predict\n",
    "    prediction_proba = loaded_model.predict(padded_sequence)[0]\n",
    "    \n",
    "    # Get class with highest probability\n",
    "    prediction_label = class_labels_deep_learning[np.argmax(prediction_proba)]\n",
    "    probabilities = dict(zip(class_labels_deep_learning, prediction_proba))\n",
    "\n",
    "    print(f\"\\nTweet: '{text}'\")\n",
    "    print(f\"Predicted Sentiment: -> {prediction_label} <-\")\n",
    "    print(\"Probabilities:\")\n",
    "    for sentiment, prob in probabilities.items():\n",
    "        print(f\"  - {sentiment}: {prob:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# ### 6.3 Test Cases\n",
    "\n",
    "predict_new_tweet(\"I am so happy with their service, it was an amazing journey!\")\n",
    "predict_new_tweet(\"The plane was dirty and the staff was rude. Never flying with them again.\")\n",
    "predict_new_tweet(\"My flight from JFK to LAX is on time.\")\n",
    "predict_new_tweet(\"@AmericanAir you are the worst. My flight is delayed again!\")\n",
    "```eof"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
